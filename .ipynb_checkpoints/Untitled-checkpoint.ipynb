{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:pytorch_pretrained_bert.modeling:loading archive file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased.tar.gz from cache at /Users/adebruijn/.pytorch_pretrained_bert/9c41111e2de84547a463fd39217199738d1e3deb72d4fec4399e6e241983c6f0.ae3cef932725ca7a30cdcb93fc6e09150a55e2a130ec7af63975a16c153ae2ba\n",
      "INFO:pytorch_pretrained_bert.modeling:extracting archive file /Users/adebruijn/.pytorch_pretrained_bert/9c41111e2de84547a463fd39217199738d1e3deb72d4fec4399e6e241983c6f0.ae3cef932725ca7a30cdcb93fc6e09150a55e2a130ec7af63975a16c153ae2ba to temp dir /var/folders/_z/cmpd3s213g95yywl93cyfvdc0000gq/T/tmpsl9dfria\n",
      "INFO:pytorch_pretrained_bert.modeling:Model config {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "INFO:pytorch_pretrained_bert.tokenization:loading vocabulary file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt from cache at /Users/adebruijn/.pytorch_pretrained_bert/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n",
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'all_encoded_layers' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-a993008a8ee8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    134\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mfirst_layer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msecond_to_last\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtoken_last_four_sum\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtoken_last_four_cat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtoken_sum_all\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    135\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 136\u001b[0;31m \u001b[0mfirst\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msecond_to_last\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlast_four_sum\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlast_four_cat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msum_all\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_embedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_encoded_layers\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    137\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m \u001b[0;31m# col_names = ['first', 'second_to_last', 'last_four_sum', 'last_four_cat', 'sum_all']\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'all_encoded_layers' is not defined"
     ]
    }
   ],
   "source": [
    "### HIDDEN STATES ###\n",
    "import torch\n",
    "from pytorch_pretrained_bert import BertTokenizer, BertModel, BertForMaskedLM\n",
    "import pandas as pd\n",
    "\n",
    "# OPTIONAL: if you want to have more information on what's happening, activate the logger as follows\n",
    "import logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "# Load pre-trained model (weights)\n",
    "model = BertModel.from_pretrained('bert-base-uncased')\n",
    "# Load pre-trained model tokenizer (vocabulary)\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Text\n",
    "from data_loader import load_data, clean_data\n",
    "\n",
    "subtask = 'a'\n",
    "texts = [\"Here is the sentence I want embeddings for.\",\n",
    "\"After stealing money from the bank vault, the bank robber was seen fishing on the Mississippi river bank.\"]\n",
    "\n",
    "train, val, test = load_data(subtask)\n",
    "\n",
    "#Clean data\n",
    "X_train, y_train = clean_data(train)\n",
    "X_val, y_val = clean_data(val)\n",
    "X_test, y_test = clean_data(test)\n",
    "\n",
    "max_seq_length = 40\n",
    "def get_features(texts):\n",
    "\n",
    "    col_names = [\"tokenized_text\",\"tokens_tensor\",\"segments_tensors\"]\n",
    "    features = pd.DataFrame(columns=col_names)\n",
    "\n",
    "    for text in texts:\n",
    "        marked_text = \"[CLS] \" + text + \" [SEP]\"\n",
    "\n",
    "        #print(\"Text: \", text)\n",
    "\n",
    "        # Tokenization\n",
    "        tokenized_text = tokenizer.tokenize(marked_text)\n",
    "        #print(\"Tokenized text: \", tokenized_text)\n",
    "\n",
    "        # Convert token to vocabulary indices\n",
    "        indexed_tokens = tokenizer.convert_tokens_to_ids(tokenized_text)\n",
    "\n",
    "        # Segement IDs\n",
    "        segments_ids = [1] * len(tokenized_text)\n",
    "\n",
    "        # Padding\n",
    "\n",
    "        padding = [0] * (max_seq_length - len(indexed_tokens))\n",
    "\n",
    "        indexed_tokens += padding\n",
    "        segments_ids += padding\n",
    "\n",
    "        # Convert inputs to PyTorch tensors\n",
    "        tokens_tensor = torch.tensor([indexed_tokens])\n",
    "        segments_tensors = torch.tensor([segments_ids])\n",
    "\n",
    "        input_features = {'tokenized_text': tokenized_text, 'tokens_tensor':tokens_tensor, 'segments_tensors':segments_tensors}\n",
    "        features.loc[len(features)] = input_features\n",
    "\n",
    "    return features\n",
    "\n",
    "features = get_features(X_test)\n",
    "# val_features = get_features(X_val)\n",
    "# test_features = get_features(X_test)\n",
    "#print(features)\n",
    "\n",
    "### Predict hidden states features for each layer ###\n",
    "model.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    # all_encoded_layers = []\n",
    "\n",
    "    # for i  in range(len(features.index)):\n",
    "        #print(i)\n",
    "    encoded_layers, _ = model(features['tokens_tensor'][0], features['segments_tensors'][0])\n",
    "        # all_encoded_layers.append(encoded_layers)\n",
    "\n",
    "\n",
    "def get_embedding(encoded_layers):\n",
    "    token_embeddings = []\n",
    "    for token_i in range(max_seq_length):\n",
    "\n",
    "        hidden_layers = []\n",
    "\n",
    "        for layer_i in range(len(encoded_layers)):\n",
    "            vec = encoded_layers[layer_i][0][token_i]\n",
    "\n",
    "            hidden_layers.append(vec)\n",
    "\n",
    "    token_embeddings.append(hidden_layers)\n",
    "\n",
    "    ### First Layer ###\n",
    "    first_layer = torch.mean(encoded_layers[0], 1)\n",
    "    # print(len(first_layer), len(first_layer[0]))\n",
    "\n",
    "    ### Second-to-Last Hidden ###\n",
    "    second_to_last = torch.mean(encoded_layers[11], 1)\n",
    "\n",
    "    # print(len(second_to_last), len(second_to_last[0]))\n",
    "\n",
    "    ### Sum Last Four Hidden ###\n",
    "    # token_last_four_sum = []\n",
    "    #\n",
    "    # for token in token_embeddings:\n",
    "    #     sum_vec = torch.sum(torch.stack(token)[-4:], 0)\n",
    "    #     token_last_four_sum.append(sum_vec)\n",
    "    #\n",
    "    # print(len(token_last_four_sum), len(token_last_four_sum[0]))\n",
    "    token_last_four_sum = [torch.sum(torch.stack(token)[-4:], 0) for token in token_embeddings]\n",
    "\n",
    "    ### Concat Last Four Hidden ###\n",
    "    # token_last_four_cat = []\n",
    "    # for token in token_embeddings:\n",
    "    #     cat_vec = torch.cat((token[-1], token[-2], token[-3], token[-4]), 0)\n",
    "    #     token_last_four_cat.append(cat_vec)\n",
    "    #\n",
    "    # print(len(token_last_four_cat), len(token_last_four_cat[0]))\n",
    "    token_last_four_cat = [torch.cat((token[-1], token[-2], token[-3], token[-4]), 0) for token in token_embeddings]\n",
    "\n",
    "    ### Sum All 12 Layers ###\n",
    "    # token_sum_all = []\n",
    "    #\n",
    "    # for token in token_embeddings:\n",
    "    #     sum_vec = torch.sum(torch.stack(token)[0:], 0)\n",
    "    #     token_sum_all.append(sum_vec)\n",
    "    #\n",
    "    # print(len(token_sum_all), len(token_sum_all[0]))\n",
    "    token_sum_all = [torch.sum(torch.stack(token)[0:], 0) for token in token_embeddings]\n",
    "\n",
    "    return first_layer, second_to_last, token_last_four_sum, token_last_four_cat, token_sum_all\n",
    "\n",
    "first, second_to_last, last_four_sum, last_four_cat, sum_all = get_embedding(all_encoded_layers[0])\n",
    "\n",
    "# col_names = ['first', 'second_to_last', 'last_four_sum', 'last_four_cat', 'sum_all']\n",
    "# embeddings = pd.DataFrame(columns=col_names)\n",
    "\n",
    "# for i, encoded_layer in enumerate(all_encoded_layers):\n",
    "#     print(i)\n",
    "#     first, second_to_last, last_four_sum, last_four_cat, sum_all = get_embedding(encoded_layer)\n",
    "\n",
    "#     input_embeddings = {'first': first, 'second_to_last':second_to_last, 'last_four_sum':last_four_sum, 'last_four_cat':last_four_cat, 'sum_all':sum_all}\n",
    "#     embeddings.loc[len(embeddings)] = input_embeddings\n",
    "\n",
    "# # print(embeddings)\n",
    "\n",
    "# embeddings.to_csv(\"data/test_embeddings_bert.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "second_to_last"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "last_four_sum"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
