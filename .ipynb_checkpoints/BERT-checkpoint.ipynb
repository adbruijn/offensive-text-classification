{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import numpy as np\n",
    "from sklearn.metrics import recall_score, precision_score, f1_score, classification_report\n",
    "\n",
    "import json\n",
    "import logging\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "import torch\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import recall_score, precision_score, f1_score, classification_report\n",
    "import torch\n",
    "\n",
    "import re\n",
    "import contractions\n",
    "import emoji\n",
    "import string\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from wordcloud import STOPWORDS\n",
    "import numpy as np\n",
    "\n",
    "from nltk.stem.porter import *\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "import emoji\n",
    "import string\n",
    "from textblob import TextBlob\n",
    "\n",
    "stopwords = stopwords.words('english')\n",
    "\n",
    "#Features\n",
    "def generate_features(df):\n",
    "\n",
    "    features = pd.DataFrame({'text':df['text']})\n",
    "\n",
    "    features['emojis'] = features[\"text\"].apply(lambda x: emoji.emoji_count(x))\n",
    "    features['urls'] = features[\"text\"].apply(lambda x: len(re.findall(\"URL\", str(x))))\n",
    "    features['hashtags'] = features[\"text\"].apply(lambda x: len(re.findall(\"#\", str(x))))\n",
    "    features['users'] = features[\"text\"].apply(lambda x: len(re.findall(\"USER\", str(x))))\n",
    "\n",
    "    features['words'] = features['text'].apply(lambda x: len(str(x).split(\" \")))\n",
    "    features['unique_words'] = features['text'].apply(lambda x: len(set(str(x).split(\" \"))))\n",
    "    features['chars'] = features['text'].str.len()\n",
    "    features['stopwords'] = features['text'].apply(lambda x: len([x for x in x.split() if x in stopwords]))\n",
    "    features['punctuations'] = features[\"text\"].apply(lambda x: len([c for c in str(x) if c in string.punctuation]))-features['users']-features['hashtags']\n",
    "    features['numerics'] = features['text'].apply(lambda x: len([x for x in x.split() if x.isdigit()]))\n",
    "\n",
    "    features['upper'] = features['text'].apply(lambda x: len([x for x in x.split() if x.isupper()]))-features['urls']-features['users']\n",
    "    features['title'] = features['text'].apply(lambda x: len([x for x in x.split() if x.istitle()]))\n",
    "\n",
    "    features['polarity'] = features['text'].apply(lambda x: np.round(TextBlob(x).sentiment.polarity, 2))\n",
    "    features['subjectivity'] = features['text'].apply(lambda x: np.round(TextBlob(x).sentiment.subjectivity, 2))\n",
    "\n",
    "    features = features.drop(columns='text', axis=1)\n",
    "\n",
    "    return features\n",
    "\n",
    "#Metrics\n",
    "def accuracy_recall_precision_f1(y_pred, y_target):\n",
    "\n",
    "    \"\"\"Computes the accuracy, recall, precision and f1 score for given predictions and targets\n",
    "    Args:\n",
    "        y_pred: Logits of the predictions for each class\n",
    "        y_target: Target values\n",
    "    \"\"\"\n",
    "\n",
    "    y_pred = y_pred.cpu()\n",
    "    y_target = y_target.cpu().numpy()\n",
    "\n",
    "    predictions = torch.argmax(y_pred, dim=1).detach().numpy()\n",
    "    #predictions = torch.max(y_pred, 1)[1].view(y_target.size()).data\n",
    "\n",
    "    correct = np.sum(predictions == y_target)\n",
    "    accuracy = correct / len(predictions)\n",
    "\n",
    "    recall = recall_score(y_target, predictions, average=None)\n",
    "    precision = precision_score(y_target, predictions, average=None)\n",
    "    f1 = f1_score(y_target, predictions, average=None)\n",
    "\n",
    "    return accuracy, recall, precision, f1\n",
    "\n",
    "#Preprocess Data\n",
    "def clean_text(text, remove_punt_number_special_chars=False,remove_stopwords=False, apply_stemming=False):\n",
    "    \"\"\"Clean text\n",
    "    Args:\n",
    "        text: (str) Text\n",
    "        remove_punt_number_special_chars: (bool) Remove punctuations, numbers and special characters\n",
    "        remove_stopwords: (bool) Remove stopwords\n",
    "        apply_stemming: (bool) Apply stemming on the words on the text\n",
    "    \"\"\"\n",
    "    #Remove emojis\n",
    "    text = re.sub(\":[a-zA-Z\\-\\_]*:\",\"\", emoji.demojize(text)) #:hear-no-evil_monkey:\n",
    "    text = re.sub(\":\\w+:\",\"\", emoji.demojize(text))\n",
    "    text = re.sub(\":\\w+\\â€™\\w+:\",\"\", emoji.demojize(text)) #:woman's_boot:\n",
    "\n",
    "    #Remove mentions, usernames (@USER)\n",
    "    text = re.sub(\"\\s*@USER\\s*\", '', text)\n",
    "\n",
    "    #Remove URL\n",
    "    text = re.sub(\"\\s*URL\\s*\", '', text)\n",
    "\n",
    "    #And\n",
    "    text = re.sub(\"&amp;\", \"and\", text)\n",
    "    text = re.sub(\"&lt;\", \"<\", text)\n",
    "    text = re.sub(\"&gt\", \">\", text)\n",
    "    text = re.sub(\"&\", \"and\", text)\n",
    "\n",
    "    #Replace contractions and slang of word\n",
    "    text = re.sub(\"i'm\", \"I'm\", text)\n",
    "    text = contractions.fix(text, slang=True)\n",
    "\n",
    "    #Lowercase\n",
    "    text = text.lower()\n",
    "\n",
    "    #Remove Hashtags + Words\n",
    "    text = re.sub(\"#\\s*\\w+\\s*\", '', text)\n",
    "\n",
    "    #Remove repeating whitespaces\n",
    "    text = re.sub(\"\\s[2, ]\",\" \", text)\n",
    "\n",
    "    #Remove non ascii characters\n",
    "    text.encode(\"ascii\", errors=\"ignore\").decode()\n",
    "\n",
    "    #Remove punctuations, numbers and special characters (remove emoticons)\n",
    "    if remove_punt_number_special_chars:\n",
    "        text = re.sub('[^a-zA-Z]', ' ', text)\n",
    "\n",
    "    #Tokenize text\n",
    "    tt = TweetTokenizer(preserve_case=False,\n",
    "                    strip_handles=True,\n",
    "                    reduce_len=True)\n",
    "\n",
    "    text_tokens = tt.tokenize(text)\n",
    "\n",
    "    #Remove stopwords\n",
    "    if remove_stopwords:\n",
    "        stopwords = set(STOPWORDS)\n",
    "        text_tokens = [token for token in text_tokens if token not in stopwords]\n",
    "\n",
    "    #Stemming\n",
    "    if apply_stemming:\n",
    "        text_stem = [stemmer.stem(token) for token in text_tokens]\n",
    "\n",
    "    clean = \" \".join(text_tokens)\n",
    "\n",
    "    return clean\n",
    "\n",
    "#BERT\n",
    "def convert_examples_to_features(X, y, max_seq_length, tokenizer):\n",
    "\n",
    "    \"\"\"Loads a data file and returns examples (input_ids, input_mask, segment_ids, label_id).\n",
    "    Args:\n",
    "        data: Data\n",
    "        max_seq_length: (int) Maximum length of the sequences\n",
    "        tokenizer: Tokenizer\n",
    "    \"\"\"\n",
    "\n",
    "    col_names = [\"input_ids\",\"input_mask\",\"segment_ids\",\"label_id\"]\n",
    "    features = pd.DataFrame(columns=col_names)\n",
    "\n",
    "    df = pd.DataFrame({\"text\":X, \"label\":y})\n",
    "\n",
    "    for index, example in df.iterrows():\n",
    "\n",
    "        tokens_text = tokenizer.tokenize(example.text)\n",
    "\n",
    "        # Account for [CLS] and [SEP] with \"- 2\"\n",
    "        if len(tokens_text) > max_seq_length - 2:\n",
    "            tokens_text = tokens_text[:(max_seq_length - 2)]\n",
    "\n",
    "        tokens = [\"[CLS]\"] + tokens_text + [\"[SEP]\"]\n",
    "        segment_ids = [0] * len(tokens)\n",
    "        input_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "\n",
    "        # The mask has 1 for real tokens and 0 for padding tokens. Only real tokens are attended to.\n",
    "        input_mask = [1] * len(input_ids)\n",
    "\n",
    "        # Zero-pad up to the sequence length.\n",
    "        padding = [0] * (max_seq_length - len(input_ids))\n",
    "\n",
    "        input_ids += padding\n",
    "        input_mask += padding\n",
    "        segment_ids += padding\n",
    "\n",
    "        label_id = int(example.label)\n",
    "\n",
    "        input_features = {'input_ids': input_ids, 'input_mask':input_mask, 'segment_ids':segment_ids, 'label_id':label_id}\n",
    "        features.loc[len(features)] = input_features\n",
    "\n",
    "    return features\n",
    "\n",
    "#Checkpoints save and load\n",
    "def save_checkpoint(state, directory, checkpoint):\n",
    "\n",
    "    \"\"\"Saves model and training parameters at checkpoint\n",
    "    Args:\n",
    "        state: (dict) contains model's state_dict, may contain other keys such as epoch, optimizer state_dict\n",
    "        checkpoint: (string) folder where parameters are to be saved\n",
    "    \"\"\"\n",
    "\n",
    "    filepath = directory + checkpoint\n",
    "\n",
    "    if not os.path.exists(directory):\n",
    "        os.makedirs(directory)\n",
    "\n",
    "    torch.save(state, filepath)\n",
    "\n",
    "\n",
    "def load_checkpoint(checkpoint, model, optimizer=None):\n",
    "\n",
    "    \"\"\"Loads model parameters (state_dict) from file_path. If optimizer is provided, loads state_dict of\n",
    "    optimizer assuming it is present in checkpoint.\n",
    "    Args:\n",
    "        checkpoint: (string) filename which needs to be loaded\n",
    "        model: (torch.nn.Module) model for which the parameters are loaded\n",
    "        optimizer: (torch.optim) optional: resume optimizer from checkpoint\n",
    "    \"\"\"\n",
    "\n",
    "    if not os.path.exists(checkpoint):\n",
    "        raise (\"File doesn't exist {}\".format(checkpoint))\n",
    "    checkpoint = torch.load(checkpoint)\n",
    "    model.load_state_dict(checkpoint['state_dict'])\n",
    "\n",
    "    if optimizer:\n",
    "        optimizer.load_state_dict(checkpoint['optim_dict'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "from collections import OrderedDict\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import recall_score, precision_score, f1_score, classification_report\n",
    "import torch\n",
    "\n",
    "import re\n",
    "import contractions\n",
    "import emoji\n",
    "import string\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from wordcloud import STOPWORDS\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchtext import data\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import (DataLoader, RandomSampler, SequentialSampler,\n",
    "                              TensorDataset)\n",
    "\n",
    "from utils import convert_examples_to_features\n",
    "from utils import clean_text\n",
    "\n",
    "from pytorch_pretrained_bert.file_utils import PYTORCH_PRETRAINED_BERT_CACHE, WEIGHTS_NAME, CONFIG_NAME\n",
    "from pytorch_pretrained_bert.modeling import BertForSequenceClassification, BertConfig\n",
    "from pytorch_pretrained_bert.tokenization import BertTokenizer\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from utils import generate_features\n",
    "\n",
    "def load_glove(embedding_file):\n",
    "\n",
    "    \"\"\"Load GloVe file\n",
    "    Args:\n",
    "        embedding_file: (str) Directory of the embedding file\n",
    "    \"\"\"\n",
    "\n",
    "    EMBEDDING_FILE = embedding_file\n",
    "    embeddings_index = dict()\n",
    "\n",
    "    for line in open(EMBEDDING_FILE):\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        coefs = np.asarray(values[1:], dtype='float32')\n",
    "        embeddings_index[word] = coefs\n",
    "    print(\"Loaded {} word vectors\".format(len(embeddings_index)))\n",
    "\n",
    "    return embeddings_index\n",
    "\n",
    "def create_weight_matrix(vocab_size, word_index, embedding_dim, embeddings_index):\n",
    "\n",
    "    \"\"\"Create weight matrix for the embeddings\n",
    "    Args:\n",
    "        vocab_size: Vocabulary size\n",
    "        word_index: Word index\n",
    "        embedding_dim: Dimension of the embeddings\n",
    "        embeddings_index: Index of the embedding\n",
    "    \"\"\"\n",
    "    embedding_matrix = np.zeros((vocab_size, embedding_dim))\n",
    "    for word, i in word_index.items():\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[i] = embedding_vector\n",
    "        else:\n",
    "            embedding_vector = embeddings_index.get(word.capitalize())\n",
    "\n",
    "            if embedding_vector is not None:\n",
    "                embedding_matrix[i] = embedding_vector\n",
    "\n",
    "    return embedding_matrix\n",
    "\n",
    "#Load data\n",
    "def load_data():\n",
    "    \"\"\"\n",
    "    Loads the data, if the data is not splitted yet the data will be split in a train and val set\n",
    "    \"\"\"\n",
    "\n",
    "    RANDOM_STATE = 123\n",
    "\n",
    "    train_file = Path(\"data/train.csv\")\n",
    "\n",
    "    if train_file.exists():\n",
    "        train = pd.read_csv(\"data/train.csv\")\n",
    "        val = pd.read_csv(\"data/val.csv\")\n",
    "        test = pd.read_csv(\"data/test.csv\")\n",
    "    else:\n",
    "        # Split normal data\n",
    "        train_cola = pd.read_csv(\"data/SemEval/olid-training-v1.0.tsv\", delimiter=\"\\t\")\n",
    "        test_cola = pd.read_csv(\"data/SemEval/testset-levela.tsv\", delimiter=\"\\t\")\n",
    "        labels_cola = pd.read_csv(\"data/SemEval/labels-levela.csv\", header=None)\n",
    "        labels_cola.columns = ['id', 'subtask_a']\n",
    "\n",
    "        test = pd.merge(test_cola, labels_cola, on='id')\n",
    "\n",
    "        # Remove duplicates\n",
    "        train_cola = train_cola.drop_duplicates(\"tweet\")\n",
    "        test = test.drop_duplicates(\"tweet\")\n",
    "\n",
    "        train, val = train_test_split(train_cola, test_size=0.2, random_state=RANDOM_STATE)\n",
    "        train.reset_index(drop=True)\n",
    "        val.reset_index(drop=True)\n",
    "\n",
    "        train = train[[\"tweet\",\"subtask_a\"]]\n",
    "        val = val[[\"tweet\",\"subtask_a\"]]\n",
    "        test = test[[\"tweet\",\"subtask_a\"]]\n",
    "\n",
    "        train.columns = ['text', 'label']\n",
    "        val.columns = ['text','label']\n",
    "        test.columns = ['text', 'label']\n",
    "\n",
    "        train.to_csv(\"data/train.csv\", index=False)\n",
    "        val.to_csv(\"data/val.csv\", index=False)\n",
    "        test.to_csv(\"data/test.csv\", index=False)\n",
    "\n",
    "    return train, val, test\n",
    "\n",
    "def load_data_features():\n",
    "    \"\"\"\n",
    "    Loads the data, if the data is not splitted yet the data will be split in a train and val set\n",
    "    \"\"\"\n",
    "\n",
    "    RANDOM_STATE = 123\n",
    "\n",
    "    train_file = Path(\"data/train.csv\")\n",
    "\n",
    "    if train_file.exists():\n",
    "        train = pd.read_csv(\"data/train.csv\")\n",
    "        val = pd.read_csv(\"data/val.csv\")\n",
    "        test = pd.read_csv(\"data/test.csv\")\n",
    "\n",
    "        features_train = pd.read_csv(\"data/features_train.csv\")\n",
    "        features_val = pd.read_csv(\"data/features_val.csv\")\n",
    "        features_test = pd.read_csv(\"data/features_test.csv\")\n",
    "\n",
    "    else:\n",
    "        # Split normal data\n",
    "        train_cola = pd.read_csv(\"data/SemEval/olid-training-v1.0.tsv\", delimiter=\"\\t\")\n",
    "        test_cola = pd.read_csv(\"data/SemEval/testset-levela.tsv\", delimiter=\"\\t\")\n",
    "        labels_cola = pd.read_csv(\"data/SemEval/labels-levela.csv\", header=None)\n",
    "        labels_cola.columns = ['id', 'subtask_a']\n",
    "\n",
    "        test = pd.merge(test_cola, labels_cola, on='id')\n",
    "\n",
    "        # Remove duplicates\n",
    "        train_cola = train_cola.drop_duplicates(\"tweet\")\n",
    "        test = test.drop_duplicates(\"tweet\")\n",
    "\n",
    "        train, val = train_test_split(train_cola, test_size=0.2, random_state=RANDOM_STATE)\n",
    "        train.reset_index(drop=True)\n",
    "        val.reset_index(drop=True)\n",
    "\n",
    "        train = train[[\"tweet\",\"subtask_a\"]]\n",
    "        val = val[[\"tweet\",\"subtask_a\"]]\n",
    "        test = test[[\"tweet\",\"subtask_a\"]]\n",
    "\n",
    "        train.columns = ['text', 'label']\n",
    "        val.columns = ['text','label']\n",
    "        test.columns = ['text', 'label']\n",
    "\n",
    "        # Generate features\n",
    "        features_train = generate_features(train)\n",
    "        features_val = generate_features(val)\n",
    "        features_test = generate_features(test)\n",
    "\n",
    "        train.to_csv(\"data/train.csv\", index=False)\n",
    "        val.to_csv(\"data/val.csv\", index=False)\n",
    "        test.to_csv(\"data/test.csv\", index=False)\n",
    "\n",
    "        features_train.to_csv(\"data/features_train.csv\")\n",
    "        features_val.to_csv(\"data/features_val.csv\")\n",
    "        features_test.to_csv(\"data/features_test.csv\")\n",
    "\n",
    "    return train, val, test, features_train, features_val, features_test\n",
    "\n",
    "#Clean Data\n",
    "def clean_data(df, remove_punt_number_special_chars=False,remove_stopwords=False, apply_stemming=False):\n",
    "    \"\"\"Clean the data and remove data which has a length of less than 3 words\n",
    "    Args:\n",
    "        df: Dataframe\n",
    "    \"\"\"\n",
    "\n",
    "    labels = encode_label(df[\"label\"])\n",
    "    text_clean = [clean_text(text, remove_punt_number_special_chars,remove_stopwords, apply_stemming) for text in df[\"text\"]]\n",
    "\n",
    "    df = pd.DataFrame({\"text\":text_clean, \"label\":labels})\n",
    "\n",
    "    return text_clean, labels\n",
    "\n",
    "#Get Dataloader\n",
    "def get_dataloader(examples, batch_size):\n",
    "    \"\"\"Make data iterator\n",
    "        Arguments:\n",
    "            X: Features\n",
    "            y: Labels\n",
    "            batch_size: (int) Batch size\n",
    "    \"\"\"\n",
    "\n",
    "    all_input_ids = torch.tensor(list(examples.input_ids), dtype=torch.long)\n",
    "    all_input_mask = torch.tensor(list(examples.input_mask), dtype=torch.long)\n",
    "    all_segment_ids = torch.tensor(list(examples.segment_ids), dtype=torch.long)\n",
    "    all_label_ids = torch.tensor(list(examples.label_id), dtype=torch.long)\n",
    "\n",
    "    data = TensorDataset(all_input_ids, all_input_mask, all_segment_ids, all_label_ids)\n",
    "    sampler = RandomSampler(data)\n",
    "    dataloader = DataLoader(data, sampler=sampler, batch_size=batch_size)\n",
    "\n",
    "    return dataloader\n",
    "\n",
    "def make_iterator(X, y, batch_size):\n",
    "\n",
    "    \"\"\"Make iterator for a given X and y and batch size\n",
    "    Args:\n",
    "        X: X vector\n",
    "        y: y vector\n",
    "        batch_size: (int) Batch size\n",
    "    \"\"\"\n",
    "\n",
    "    X = torch.tensor(X, dtype=torch.long)\n",
    "    y = torch.tensor(y, dtype=torch.float32)\n",
    "    ds = TensorDataset(X, y)\n",
    "    loader = DataLoader(ds, batch_size=batch_size)\n",
    "\n",
    "    return loader\n",
    "\n",
    "def make_iterator_features(X, features, y, batch_size):\n",
    "\n",
    "    \"\"\"Make iterator for a given X and y and batch size\n",
    "    Args:\n",
    "        X: X vector\n",
    "        y: y vector\n",
    "        batch_size: (int) Batch size\n",
    "    \"\"\"\n",
    "\n",
    "    X = torch.tensor(X, dtype=torch.long)\n",
    "    features = torch.tensor(list(features), dtype=torch.long)\n",
    "    y = torch.tensor(y, dtype=torch.float32)\n",
    "    ds = TensorDataset(X, features, y)\n",
    "    loader = DataLoader(ds, batch_size=batch_size)\n",
    "\n",
    "    return loader\n",
    "\n",
    "def encode_label(y):\n",
    "\n",
    "    \"\"\"Encode labels from str to numbers\n",
    "    Args:\n",
    "        y: y vector\n",
    "    \"\"\"\n",
    "\n",
    "    y = y.values\n",
    "    le = LabelEncoder()\n",
    "    le.fit(y)\n",
    "\n",
    "    return np.array(le.transform(y))\n",
    "\n",
    "def get_data_bert(max_seq_length, batch_sizes):\n",
    "\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        max_num_words: (int) Max number of words as input for the Tokenizer\n",
    "        embedding_dim: (int) Embedding dim of the embeddings\n",
    "        max_seq_length: (int) Max sequence length of the sentences\n",
    "        batch_size: (int) Batch size for the DataLoader\n",
    "        use_bert: (bool) Use the BERT model or another model\n",
    "    Output:\n",
    "        word_index, embedding_matrix, X_train, y_train, X_test, y_test\n",
    "    \"\"\"\n",
    "\n",
    "    #Load data\n",
    "    train, val, test = load_data()\n",
    "\n",
    "    #Clean data\n",
    "\n",
    "    X_train, y_train = clean_data(train)\n",
    "    X_val, y_val = clean_data(val)\n",
    "    X_test, y_test = clean_data(test)\n",
    "\n",
    "    #Features data\n",
    "    tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)\n",
    "\n",
    "    train_examples = convert_examples_to_features(X_train, y_train, max_seq_length, tokenizer)\n",
    "    val_examples = convert_examples_to_features(X_val, y_val, max_seq_length, tokenizer)\n",
    "    test_examples = convert_examples_to_features(X_test, y_test, max_seq_length, tokenizer)\n",
    "\n",
    "    #Data loaders\n",
    "    train_dataloader = get_dataloader(train_examples, batch_sizes[0])\n",
    "    val_dataloader = get_dataloader(val_examples, batch_sizes[1])\n",
    "    test_dataloader = get_dataloader(test_examples, batch_sizes[2])\n",
    "\n",
    "    return train_dataloader, val_dataloader, test_dataloader\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "from torch.nn import functional as F\n",
    "import numpy as np\n",
    "\n",
    "from pytorch_pretrained_bert import BertTokenizer, BertModel, BertForSequenceClassification\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "from torch.nn import functional as F\n",
    "import numpy as np\n",
    "\n",
    "class BertLinear(nn.Module):\n",
    "    def __init__(self, hidden_dim, dropout, output_dim):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            hidden_dim: Size hiddden state\n",
    "            dropout: Dropout probability\n",
    "            output_dim: Output dimension (number of labels)\n",
    "        \"\"\"\n",
    "\n",
    "        super(BertLinear, self).__init__()\n",
    "        self.output_dim = output_dim\n",
    "        self.dropout = dropout\n",
    "\n",
    "        self.bert = BertModel.from_pretrained('bert-base-uncased')\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.relu = nn.LeakyReLU()\n",
    "\n",
    "        self.linear1 = nn.Linear(768, hidden_dim) #self.bert.config.hidden_size = 768\n",
    "        self.linear2 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.linear3 = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "\n",
    "    def forward(self, input_ids, token_type_ids=None, attention_mask=None, labels=None):\n",
    "\n",
    "        encoded_layers, pooled_output = self.bert(input_ids, token_type_ids, attention_mask, output_all_encoded_layers=False)\n",
    "\n",
    "        x = self.relu(self.linear1(pooled_output))\n",
    "        x = self.dropout(x)\n",
    "        x = self.relu(self.linear2(x))\n",
    "        x = self.relu(self.linear3(x))\n",
    "\n",
    "        return x\n",
    "\n",
    "class BertLSTM(nn.Module):\n",
    "    def __init__(self, hidden_dim, dropout, output_dim):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            dropout: Dropout probability\n",
    "            output_dim: Output dimension (number of labels)\n",
    "        \"\"\"\n",
    "\n",
    "        super(BertLSTM, self).__init__()\n",
    "        self.output_dim = output_dim\n",
    "        self.dropout = dropout\n",
    "\n",
    "        self.bert = BertModel.from_pretrained('bert-base-uncased')\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        self.lstm = nn.LSTM(768, hidden_dim, bidirectional=True) #self.bert.config.hidden_size = 768\n",
    "        self.output = nn.Linear(hidden_dim * 2, output_dim)\n",
    "\n",
    "\n",
    "    def forward(self, input_ids, token_type_ids=None, attention_mask=None, labels=None):\n",
    "        encoded_layers, pooled_output = self.bert(input_ids, token_type_ids, attention_mask, output_all_encoded_layers=False)\n",
    "        encoded_layers = encoded_layers.permute(1, 0 ,2)\n",
    "\n",
    "        output, (hidden_state, cell_state) = self.lstm(encoded_layers)\n",
    "\n",
    "        out = torch.cat((hidden_state[0], hidden_state[1]), dim=1)\n",
    "\n",
    "        out = self.dropout(out)\n",
    "\n",
    "        x = self.output(out)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import logging\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm, trange\n",
    "\n",
    "from utils import accuracy_recall_precision_f1\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "def train_model(model, optimizer, loss_fn, dataloader, device, use_bert):\n",
    "    \"\"\"Train model\n",
    "    Args:\n",
    "        model: Model either LSTM, LSTMAttention, CNN, MLP (torch.nn.Module)\n",
    "        optimizer: Optimizer for parameters of the model (torch.optim)\n",
    "        loss_fn: Loss function taht computs the loss for each batch based on the y_pred and y_target\n",
    "        dataloader: Dataloader that generates batches of data and labels or in case of BERT input_ids, input_mask, segment_ids and label_ids\n",
    "        device: Device run either on GPU or CPU\n",
    "    \"\"\"\n",
    "\n",
    "    #Metrics\n",
    "    epoch_loss = 0\n",
    "    epoch_accuracy = 0\n",
    "    epoch_recall = [0, 0]\n",
    "    epoch_precision = [0, 0]\n",
    "    epoch_f1 = [0, 0]\n",
    "\n",
    "    #Set model in training mode\n",
    "    model.train()\n",
    "\n",
    "    for step, batch in enumerate(tqdm(dataloader, desc=\"Iteration\")):\n",
    "\n",
    "        #Step 0: Get batch\n",
    "        batch = tuple(t.to(device) for t in batch)\n",
    "\n",
    "        if use_bert:\n",
    "            input_ids, input_mask, segment_ids, label_ids = batch\n",
    "        else:\n",
    "            X, y_target = batch\n",
    "            y_target = torch.autograd.Variable(y_target).long()\n",
    "\n",
    "        #Step 1: Clear the gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        #Step 2: Compute the forward pass of the model (model output)\n",
    "        if use_bert:\n",
    "            y_pred = model(input_ids, segment_ids, input_mask, labels=None)\n",
    "            y_target = label_ids\n",
    "        else:\n",
    "            y_pred = model(X)\n",
    "\n",
    "        #Step 3: Compute the loss\n",
    "        loss = loss_fn(y_pred, y_target)\n",
    "        loss_batch = loss.item()\n",
    "        epoch_loss += loss_batch\n",
    "\n",
    "        #Step 4: Propagate the loss backward\n",
    "        loss.backward()\n",
    "\n",
    "        #Step 5: Use optimizer to take gradient step\n",
    "        optimizer.step()\n",
    "\n",
    "        #Compute other metrics\n",
    "        accuracy, recall, precision, f1 = accuracy_recall_precision_f1(y_pred, y_target)\n",
    "\n",
    "        epoch_accuracy += accuracy\n",
    "        epoch_recall += recall\n",
    "        epoch_precision += precision\n",
    "        epoch_f1 += f1\n",
    "\n",
    "    #Train results\n",
    "    results = {\n",
    "        'loss': np.round(epoch_loss / len(dataloader),2),\n",
    "        'accuracy': np.round(float(epoch_accuracy / len(dataloader)),2),\n",
    "        'recall': np.round(epoch_recall / len(dataloader), 2),\n",
    "        'precision': np.round(epoch_precision / len(dataloader), 2),\n",
    "        'f1': np.round(epoch_f1 / len(dataloader), 2)\n",
    "    }\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import logging\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm, trange\n",
    "\n",
    "from utils import accuracy_recall_precision_f1\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "def evaluate_model(model, optimizer, loss_fn, dataloader, device, use_bert):\n",
    "    \"\"\"Evaluate model\n",
    "    Args:\n",
    "        model: Model either LSTM, LSTMAttention, CNN, MLP (torch.nn.Module)\n",
    "        optimizer: Optimizer for parameters of the model (torch.optim)\n",
    "        loss_fn: Loss function taht computs the loss for each batch based on the y_pred and y_target\n",
    "        dataloader: Dataloader that generates batches of data and labels or in case of BERT input_ids, input_mask, segment_ids and label_ids\n",
    "        device: Device run either on GPU or CPU\n",
    "    \"\"\"\n",
    "\n",
    "    #Metrics\n",
    "    epoch_loss = 0\n",
    "    epoch_accuracy = 0\n",
    "    epoch_recall = [0, 0]\n",
    "    epoch_precision = [0, 0]\n",
    "    epoch_f1 = [0, 0]\n",
    "\n",
    "    #Set model in evaluate mode\n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "\n",
    "        for step, batch in enumerate(tqdm(dataloader, desc=\"Iteration\")):\n",
    "\n",
    "            #Step 0: Get batch\n",
    "            batch = tuple(t.to(device) for t in batch)\n",
    "\n",
    "            if use_bert:\n",
    "                input_ids, input_mask, segment_ids, label_ids = batch\n",
    "            else:\n",
    "                X, y_target = batch\n",
    "                y_target = torch.autograd.Variable(y_target).long()\n",
    "\n",
    "            #Step 1: Compute the forward pass of the model (model output)\n",
    "            if use_bert:\n",
    "                y_pred = model(input_ids, segment_ids, input_mask, labels=None)\n",
    "                y_target = label_ids\n",
    "            else:\n",
    "                y_pred = model(X)\n",
    "\n",
    "            #Step 2: Compute the loss\n",
    "            loss = loss_fn(y_pred, y_target)\n",
    "            loss_batch = loss.item()\n",
    "            epoch_loss += loss_batch\n",
    "\n",
    "            #Compute other metrics\n",
    "            accuracy, recall, precision, f1 = accuracy_recall_precision_f1(y_pred, y_target)\n",
    "\n",
    "            epoch_accuracy += accuracy\n",
    "            epoch_recall += recall\n",
    "            epoch_precision += precision\n",
    "            epoch_f1 += f1\n",
    "\n",
    "        #Evaluation results\n",
    "        results = {\n",
    "            'loss': np.round(epoch_loss / len(dataloader),2),\n",
    "            'accuracy': np.round(float(epoch_accuracy / len(dataloader)),2),\n",
    "            'recall': np.round(epoch_recall / len(dataloader), 2),\n",
    "            'precision': np.round(epoch_precision / len(dataloader), 2),\n",
    "            'f1': np.round(epoch_f1 / len(dataloader), 2)\n",
    "        }\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import logging\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import logging\n",
    "\n",
    "import click\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "import torch.optim as optim\n",
    "from torch.nn import CrossEntropyLoss, MSELoss\n",
    "from torch.nn.utils.rnn import pack_padded_sequence\n",
    "from torch.utils.data import (DataLoader, RandomSampler, SequentialSampler,\n",
    "                              TensorDataset)\n",
    "\n",
    "from tqdm import tqdm, trange\n",
    "\n",
    "from pytorch_pretrained_bert.file_utils import PYTORCH_PRETRAINED_BERT_CACHE, WEIGHTS_NAME, CONFIG_NAME\n",
    "from pytorch_pretrained_bert.modeling import BertForSequenceClassification, BertConfig\n",
    "from pytorch_pretrained_bert.tokenization import BertTokenizer\n",
    "from pytorch_pretrained_bert.optimization import BertAdam, WarmupLinearSchedule\n",
    "\n",
    "from train import train_model\n",
    "from evaluate import evaluate_model\n",
    "from utils import accuracy_recall_precision_f1, save_checkpoint, load_checkpoint\n",
    "from data_loader import get_data, get_data_bert\n",
    "import models\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "#Sacred\n",
    "#Sources\n",
    "#https://github.com/gereleth/kaggle-telstra/blob/master/Automatic%20model%20tuning%20with%20Sacred%20and%20Hyperopt.ipynb\n",
    "#https://github.com/maartjeth/sacred-example-pytorch\n",
    "from sacred import Experiment\n",
    "from sacred.observers import FileStorageObserver\n",
    "from sacred.observers import MongoObserver\n",
    "from sacred.observers import SlackObserver\n",
    "from sacred.utils import apply_backspaces_and_linefeeds\n",
    "\n",
    "EXPERIMENT_NAME = 'experiment'\n",
    "DATABASE_NAME = 'experiments'\n",
    "URL_NAME = 'mongodb://localhost:27017/'\n",
    "\n",
    "ex = Experiment()\n",
    "ex.observers.append(FileStorageObserver.create('results'))\n",
    "#ex.observers.append(MongoObserver.create(url=URL_NAME, db_name=DATABASE_NAME))\n",
    "ex.captured_out_filter = apply_backspaces_and_linefeeds\n",
    "\n",
    "#Send a message to slack if the run is succesfull or if it failed\n",
    "slack_obs = SlackObserver.from_config('slack.json')\n",
    "ex.observers.append(slack_obs)\n",
    "\n",
    "#Device\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "def log_scalars(results, name_dataset):\n",
    "\n",
    "    \"\"\"Log scalars of the results for MongoDB and Omniboard\n",
    "    Args:\n",
    "        results: Results with the loss, accuracy, recall, precision and f1-score\n",
    "        name_dataset: The name of the dataset so it can store the scalers by name\n",
    "    \"\"\"\n",
    "\n",
    "    ex.log_scalar(name_dataset+'.loss', float(results['loss']))\n",
    "    ex.log_scalar(name_dataset+'.accuracy', float(results['accuracy']))\n",
    "    ex.log_scalar(name_dataset+'.recall.OFF', float(results['recall'][0]))\n",
    "    ex.log_scalar(name_dataset+'.recall.NOT', float(results['recall'][1]))\n",
    "    ex.log_scalar(name_dataset+'.precision.OFF', float(results['precision'][0]))\n",
    "    ex.log_scalar(name_dataset+'.precision.NOT', float(results['precision'][1]))\n",
    "    ex.log_scalar(name_dataset+'.f1.OFF', float(results['f1'][0]))\n",
    "    ex.log_scalar(name_dataset+'.f1.NOT', float(results['f1'][1]))\n",
    "\n",
    "\n",
    "@ex.capture\n",
    "def train_and_evaluate(num_epochs, model, optimizer, loss_fn, train_dataloader, val_dataloader, early_stopping_criteria, directory, use_bert, use_mongo):\n",
    "\n",
    "    \"\"\"Train on training set and evaluate on evaluation set\n",
    "    Args:\n",
    "        num_epochs: Number of epochs to run the training and evaluation\n",
    "        model: Model\n",
    "        optimizer: Optimizer\n",
    "        loss_fn: Loss function\n",
    "        dataloader: Dataloader for the training set\n",
    "        val_dataloader: Dataloader for the validation set\n",
    "        scheduler: Scheduler\n",
    "        directory: Directory path name to story the logging files\n",
    "\n",
    "    Returns train and evaluation metrics with epoch, loss, accuracy, recall, precision and f1-score\n",
    "    \"\"\"\n",
    "\n",
    "    train_metrics = pd.DataFrame(columns=['epoch', 'loss', 'accuracy', 'recall', 'precision', 'f1'])\n",
    "    val_metrics = pd.DataFrame(columns=['epoch', 'loss', 'accuracy', 'recall', 'precision', 'f1'])\n",
    "\n",
    "    best_val_loss = float(\"inf\")\n",
    "\n",
    "    early_stop_step = 0\n",
    "\n",
    "    for epoch in trange(num_epochs, desc=\"Epoch\"):\n",
    "\n",
    "        ### TRAINING ###\n",
    "        train_results = train_model(model, optimizer, loss_fn, train_dataloader, device, use_bert)\n",
    "        train_metrics.loc[len(train_metrics)] = {'epoch':epoch, 'loss':train_results['loss'], 'accuracy':train_results['accuracy'], 'recall':train_results['recall'], 'precision':train_results['precision'], 'f1':train_results['f1']}\n",
    "        if use_mongo: log_scalars(train_results, \"Train\")\n",
    "\n",
    "        ### EVALUATION ###\n",
    "        val_results = evaluate_model(model, optimizer, loss_fn, val_dataloader, device, use_bert)\n",
    "        val_metrics.loc[len(val_metrics)] = {'epoch':epoch, 'loss':val_results['loss'], 'accuracy':val_results['accuracy'], 'recall':val_results['recall'], 'precision':val_results['precision'], 'f1':val_results['f1']}\n",
    "        if use_mongo: log_scalars(val_results, \"Validation\")\n",
    "\n",
    "        #Save best and latest state\n",
    "        best_model = val_results['loss'] < best_val_loss\n",
    "        #last_model = epoch == num_epochs-1\n",
    "\n",
    "        if best_model:\n",
    "            save_checkpoint({'epoch': epoch+1,\n",
    "                                   'state_dict': model.state_dict(),\n",
    "                                   'optim_dict': optimizer.state_dict()},\n",
    "                                    directory=directory,\n",
    "                                    checkpoint='best_model.pth.tar')\n",
    "\n",
    "        # if last_model:\n",
    "        #     save_checkpoint({'epoch': epoch+1,\n",
    "        #                            'state_dict': model.state_dict(),\n",
    "        #                            'optim_dict': optimizer.state_dict()},\n",
    "        #                             directory=directory,\n",
    "        #                             checkpoint='last_model.pth.tar')\n",
    "\n",
    "        #Early stopping\n",
    "        if val_results['loss'] >= best_val_loss:\n",
    "            early_stop_step += 1\n",
    "            print(\"Early stop step:\", early_stop_step)\n",
    "        else:\n",
    "            best_val_loss = val_results['loss']\n",
    "            early_stop_step = 0\n",
    "\n",
    "        stop_early = early_stop_step >= early_stopping_criteria\n",
    "\n",
    "        if stop_early:\n",
    "            print(\"Stopping early at epoch {}\".format(epoch))\n",
    "\n",
    "            #Save last model when stop early\n",
    "            # save_checkpoint({'epoch': epoch+1,\n",
    "            #                        'state_dict': model.state_dict(),\n",
    "            #                        'optim_dict': optimizer.state_dict()},\n",
    "            #                         directory=directory,\n",
    "            #                         checkpoint='last_model.pth.tar')\n",
    "\n",
    "            return train_metrics, val_metrics\n",
    "\n",
    "        print('\\n')\n",
    "        print('Train Loss: {} | Train Acc: {}'.format(train_results['loss'], train_results['accuracy']))\n",
    "        print('Valid Loss: {} | Valid Acc: {}'.format(val_results['loss'], val_results['accuracy']))\n",
    "        print('Train recall: {} | Train precision: {}'.format(train_results['recall'], train_results['precision']))\n",
    "        print('Valid recall: {} | Valid precision: {}'.format(val_results['recall'], val_results['precision']))\n",
    "\n",
    "        #Scheduler\n",
    "        #scheduler.step()\n",
    "\n",
    "    return train_metrics, val_metrics\n",
    "\n",
    "\n",
    "@ex.config\n",
    "def config():\n",
    "\n",
    "    \"\"\"Configuration\"\"\"\n",
    "\n",
    "    output_dim = 2 #Number of labels (default=2)\n",
    "    train_bs = 32.0 #Train batch size (default=32)\n",
    "    val_bs = 32.0 #Validation batch size (default=32)\n",
    "    test_bs = 32.0  #Test batch size (default=32)\n",
    "    num_epochs = 100 #Number of epochs (default=100)\n",
    "    max_seq_length = 45 #Maximum sequence length of the sentences (default=40)\n",
    "    learning_rate = 3e-5 #Learning rate for the model (default=3e-5)\n",
    "    warmup_proportion = 0.1 #Warmup proportion (default=0.1)\n",
    "    early_stopping_criteria = 50 #Early stopping criteria (default=5)\n",
    "    num_layers = 2 #Number of layers (default=2)\n",
    "    hidden_dim = 128 #Hidden layers dimension (default=128)\n",
    "    bidirectional = False #Left and right LSTM\n",
    "    dropout = 0.1 #Dropout percentage\n",
    "    filter_sizes = [2, 3, 4] #CNN\n",
    "    embedding_file = 'data/GloVe/glove.twitter.27B.200d.txt' #Embedding file\n",
    "    model_name = \"MLP\" #Model name: LSTM, BERT, MLP, CNN\n",
    "    use_mongo = True\n",
    "\n",
    "@ex.automain\n",
    "def main(output_dim,\n",
    "        train_bs,\n",
    "        val_bs,\n",
    "        test_bs,\n",
    "        num_epochs,\n",
    "        max_seq_length,\n",
    "        learning_rate,\n",
    "        warmup_proportion,\n",
    "        early_stopping_criteria,\n",
    "        num_layers,\n",
    "        hidden_dim,\n",
    "        bidirectional,\n",
    "        dropout,\n",
    "        filter_sizes,\n",
    "        embedding_file,\n",
    "        model_name,\n",
    "        use_mongo,\n",
    "        _run):\n",
    "\n",
    "    #Logger\n",
    "    #directory = f\"results/checkpoints/{_run._id}/\"\n",
    "    directory = f\"results/{_run._id}/\"\n",
    "\n",
    "    #Batch sizes\n",
    "    batch_sizes = [int(train_bs), int(val_bs), int(test_bs)]\n",
    "    batch_size = int(train_bs)\n",
    "\n",
    "    if \"BERT\" in model_name:  #Default = False, if BERT model is used then use_bert is set to True\n",
    "        use_bert = True\n",
    "    else:\n",
    "        use_bert = False\n",
    "\n",
    "    #Data\n",
    "    if use_bert:\n",
    "        train_dataloader, val_dataloader, test_dataloader = get_data_bert(int(max_seq_length), batch_sizes)\n",
    "    else:\n",
    "        embedding_dim, vocab_size, embedding_matrix, train_dataloader, val_dataloader, test_dataloader = get_data(int(max_seq_length), embedding_file=embedding_file, batch_size=batch_size)\n",
    "\n",
    "    #Model\n",
    "    if model_name==\"MLP\":\n",
    "        model = models.MLP(embedding_matrix, embedding_dim, vocab_size, int(hidden_dim), dropout, output_dim)\n",
    "    if model_name==\"MLP_Features\":\n",
    "        model = models.MLP_Features(embedding_matrix, embedding_dim, vocab_size, int(hidden_dim), 14, dropout, output_dim)\n",
    "        print(model)\n",
    "    elif model_name==\"CNN\":\n",
    "        model = models.CNN(embedding_matrix, embedding_dim, vocab_size, dropout, filter_sizes, output_dim)\n",
    "        print(model)\n",
    "    elif model_name==\"LSTM\":\n",
    "        model = models.LSTM(embedding_matrix, embedding_dim, vocab_size, int(hidden_dim), dropout, int(num_layers), bidirectional, output_dim)\n",
    "        print(model)\n",
    "    elif model_name==\"LSTMAttention\":\n",
    "        model = models.LSTMAttention(embedding_matrix, embedding_dim, vocab_size, int(hidden_dim), dropout, int(num_layers), bidirectional, output_dim)\n",
    "        print(model)\n",
    "    elif model_name==\"BERT\":\n",
    "        model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\", output_dim)\n",
    "        print(model)\n",
    "    elif model_name==\"BERTLinear\":\n",
    "        model = models.BertLinear(hidden_dim, dropout, output_dim)\n",
    "        print(model)\n",
    "    elif model_name==\"BERTLSTM\":\n",
    "        model = models.BertLSTM(hidden_dim, dropout, output_dim)\n",
    "        print(model)\n",
    "\n",
    "    model = model.to(device)\n",
    "\n",
    "    #Loss and optimizer\n",
    "    #optimizer = optim.Adam([{'params': model.parameters(), 'weight_decay': 0.1}], lr=learning_rate)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    loss_fn = F.cross_entropy\n",
    "\n",
    "    #Scheduler\n",
    "    #scheduler = optim.lr_scheduler.MultiStepLR(optimizer, milestones=[5, 50], gamma=0.1)\n",
    "\n",
    "    #Training and evaluation\n",
    "    print('Training and evaluation for {} epochs...'.format(num_epochs))\n",
    "    train_metrics, val_metrics = train_and_evaluate(num_epochs, model, optimizer, loss_fn, train_dataloader, val_dataloader, early_stopping_criteria, directory, use_bert, use_mongo)\n",
    "    train_metrics.to_csv(directory+\"train_metrics.csv\"), val_metrics.to_csv(directory+\"val_metrics.csv\")\n",
    "\n",
    "    #Test\n",
    "    print('Testing...')\n",
    "    load_checkpoint(directory+\"best_model.pth.tar\", model)\n",
    "\n",
    "    #Add artifacts\n",
    "    #ex.add_artifact(directory+\"best_model.pth.tar\")\n",
    "    #ex.add_artifact(directory+\"last_model.pth.tar\")\n",
    "\n",
    "    test_metrics = evaluate_model(model, optimizer, loss_fn, test_dataloader, device, use_bert)\n",
    "    if use_mongo: log_scalars(test_metrics,\"Test\")\n",
    "\n",
    "    test_metrics_df = pd.DataFrame(test_metrics)\n",
    "    test_metrics_df = pd.DataFrame(test_metrics, index=[\"NOT\",\"OFF\"])\n",
    "    print(test_metrics)\n",
    "    test_metrics_df.to_csv(directory+\"test_metrics.csv\")\n",
    "\n",
    "    id_nummer = f'{_run._id}'\n",
    "\n",
    "    results = {\n",
    "        'id': id_nummer,\n",
    "        'loss': np.round(np.mean(val_metrics['loss']), 4),\n",
    "        'accuracy': test_metrics['accuracy'],\n",
    "        'recall': test_metrics['recall'],\n",
    "        'precision': test_metrics['precision'],\n",
    "        'f1': test_metrics['f1'],\n",
    "        'learning_rate': learning_rate,\n",
    "        'hidden_dim': hidden_dim,\n",
    "        'status': 'ok'\n",
    "    }\n",
    "\n",
    "    return results"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
