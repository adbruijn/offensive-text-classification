{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:pytorch_pretrained_bert.modeling:loading archive file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased.tar.gz from cache at /Users/adebruijn/.pytorch_pretrained_bert/9c41111e2de84547a463fd39217199738d1e3deb72d4fec4399e6e241983c6f0.ae3cef932725ca7a30cdcb93fc6e09150a55e2a130ec7af63975a16c153ae2ba\n",
      "INFO:pytorch_pretrained_bert.modeling:extracting archive file /Users/adebruijn/.pytorch_pretrained_bert/9c41111e2de84547a463fd39217199738d1e3deb72d4fec4399e6e241983c6f0.ae3cef932725ca7a30cdcb93fc6e09150a55e2a130ec7af63975a16c153ae2ba to temp dir /var/folders/_z/cmpd3s213g95yywl93cyfvdc0000gq/T/tmpupqq8o4x\n",
      "INFO:pytorch_pretrained_bert.modeling:Model config {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "INFO:pytorch_pretrained_bert.tokenization:loading vocabulary file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt from cache at /Users/adebruijn/.pytorch_pretrained_bert/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n"
     ]
    }
   ],
   "source": [
    "### HIDDEN STATES ###\n",
    "import torch\n",
    "from pytorch_pretrained_bert import BertTokenizer, BertModel, BertForMaskedLM\n",
    "import pandas as pd\n",
    "\n",
    "# OPTIONAL: if you want to have more information on what's happening, activate the logger as follows\n",
    "import logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "# Load pre-trained model (weights)\n",
    "model = BertModel.from_pretrained('bert-base-uncased')\n",
    "# Load pre-trained model tokenizer (vocabulary)\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Text\n",
    "from data_loader import load_data, clean_data\n",
    "\n",
    "subtask = 'a'\n",
    "texts = [\"Here is the sentence I want embeddings for.\",\n",
    "\"After stealing money from the bank vault, the bank robber was seen fishing on the Mississippi river bank.\"]\n",
    "\n",
    "train, val, test = load_data(subtask)\n",
    "\n",
    "#Clean data\n",
    "X_train, y_train = clean_data(train)\n",
    "X_val, y_val = clean_data(val)\n",
    "X_test, y_test = clean_data(test)\n",
    "\n",
    "max_seq_length = 40\n",
    "def get_features(texts):\n",
    "\n",
    "    col_names = [\"tokenized_text\",\"tokens_tensor\",\"segments_tensors\"]\n",
    "    features = pd.DataFrame(columns=col_names)\n",
    "\n",
    "    for text in texts:\n",
    "        marked_text = \"[CLS] \" + text + \" [SEP]\"\n",
    "\n",
    "        #print(\"Text: \", text)\n",
    "\n",
    "        # Tokenization\n",
    "        tokenized_text = tokenizer.tokenize(marked_text)\n",
    "        #print(\"Tokenized text: \", tokenized_text)\n",
    "\n",
    "        # Convert token to vocabulary indices\n",
    "        indexed_tokens = tokenizer.convert_tokens_to_ids(tokenized_text)\n",
    "\n",
    "        # Segement IDs\n",
    "        segments_ids = [1] * len(tokenized_text)\n",
    "\n",
    "        # Padding\n",
    "\n",
    "        padding = [0] * (max_seq_length - len(indexed_tokens))\n",
    "\n",
    "        indexed_tokens += padding\n",
    "        segments_ids += padding\n",
    "\n",
    "        # Convert inputs to PyTorch tensors\n",
    "        tokens_tensor = torch.tensor([indexed_tokens])\n",
    "        segments_tensors = torch.tensor([segments_ids])\n",
    "\n",
    "        input_features = {'tokenized_text': tokenized_text, 'tokens_tensor':tokens_tensor, 'segments_tensors':segments_tensors}\n",
    "        features.loc[len(features)] = input_features\n",
    "\n",
    "    return features\n",
    "\n",
    "features = get_features(X_test)\n",
    "# val_features = get_features(X_val)\n",
    "# test_features = get_features(X_test)\n",
    "#print(features)\n",
    "\n",
    "### Predict hidden states features for each layer ###\n",
    "model.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    # all_encoded_layers = []\n",
    "\n",
    "    # for i  in range(len(features.index)):\n",
    "        #print(i)\n",
    "    encoded_layers, _ = model(features['tokens_tensor'][0], features['segments_tensors'][0])\n",
    "        # all_encoded_layers.append(encoded_layers)\n",
    "\n",
    "\n",
    "def get_embedding(encoded_layers):\n",
    "    token_embeddings = []\n",
    "    for token_i in range(max_seq_length):\n",
    "\n",
    "        hidden_layers = []\n",
    "\n",
    "        for layer_i in range(len(encoded_layers)):\n",
    "            vec = encoded_layers[layer_i][0][token_i]\n",
    "\n",
    "            hidden_layers.append(vec)\n",
    "\n",
    "    token_embeddings.append(hidden_layers)\n",
    "\n",
    "    ### First Layer ###\n",
    "    first_layer = torch.mean(encoded_layers[0], 1)\n",
    "    # print(len(first_layer), len(first_layer[0]))\n",
    "\n",
    "    ### Second-to-Last Hidden ###\n",
    "    second_to_last = torch.mean(encoded_layers[11], 1)\n",
    "\n",
    "    # print(len(second_to_last), len(second_to_last[0]))\n",
    "\n",
    "    ### Sum Last Four Hidden ###\n",
    "    # token_last_four_sum = []\n",
    "    #\n",
    "    # for token in token_embeddings:\n",
    "    #     sum_vec = torch.sum(torch.stack(token)[-4:], 0)\n",
    "    #     token_last_four_sum.append(sum_vec)\n",
    "    #\n",
    "    # print(len(token_last_four_sum), len(token_last_four_sum[0]))\n",
    "    token_last_four_sum = [torch.sum(torch.stack(token)[-4:], 0) for token in token_embeddings]\n",
    "\n",
    "    ### Concat Last Four Hidden ###\n",
    "    # token_last_four_cat = []\n",
    "    # for token in token_embeddings:\n",
    "    #     cat_vec = torch.cat((token[-1], token[-2], token[-3], token[-4]), 0)\n",
    "    #     token_last_four_cat.append(cat_vec)\n",
    "    #\n",
    "    # print(len(token_last_four_cat), len(token_last_four_cat[0]))\n",
    "    token_last_four_cat = [torch.cat((token[-1], token[-2], token[-3], token[-4]), 0) for token in token_embeddings]\n",
    "\n",
    "    ### Sum All 12 Layers ###\n",
    "    # token_sum_all = []\n",
    "    #\n",
    "    # for token in token_embeddings:\n",
    "    #     sum_vec = torch.sum(torch.stack(token)[0:], 0)\n",
    "    #     token_sum_all.append(sum_vec)\n",
    "    #\n",
    "    # print(len(token_sum_all), len(token_sum_all[0]))\n",
    "    token_sum_all = [torch.sum(torch.stack(token)[0:], 0) for token in token_embeddings]\n",
    "\n",
    "    return first_layer, second_to_last, token_last_four_sum, token_last_four_cat, token_sum_all\n",
    "\n",
    "first, second_to_last, last_four_sum, last_four_cat, sum_all = get_embedding(encoded_layers)\n",
    "\n",
    "# col_names = ['first', 'second_to_last', 'last_four_sum', 'last_four_cat', 'sum_all']\n",
    "# embeddings = pd.DataFrame(columns=col_names)\n",
    "\n",
    "# for i, encoded_layer in enumerate(all_encoded_layers):\n",
    "#     print(i)\n",
    "#     first, second_to_last, last_four_sum, last_four_cat, sum_all = get_embedding(encoded_layer)\n",
    "\n",
    "#     input_embeddings = {'first': first, 'second_to_last':second_to_last, 'last_four_sum':last_four_sum, 'last_four_cat':last_four_cat, 'sum_all':sum_all}\n",
    "#     embeddings.loc[len(embeddings)] = input_embeddings\n",
    "\n",
    "# # print(embeddings)\n",
    "\n",
    "# embeddings.to_csv(\"data/test_embeddings_bert.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.3141, -0.4490, -0.0811, -0.4260, -0.1309,  0.1133, -0.1384,  0.3867,\n",
       "          0.0093, -0.0251,  0.1767,  0.0027, -0.3276,  0.2614, -0.3140,  0.1850,\n",
       "          0.2464, -0.1266,  0.0128, -0.1044, -0.1372, -0.1791,  0.1078, -0.0380,\n",
       "          0.2169, -0.0779, -0.0572,  0.0836,  0.0460,  0.2432,  0.2034,  0.1800,\n",
       "          0.2450,  0.0862, -0.0305,  0.0457, -0.1843,  0.0195,  0.2808, -0.2686,\n",
       "          0.0086, -0.2583, -0.0541,  0.1873,  0.1133, -0.4559,  0.3318, -0.0180,\n",
       "          0.3753, -0.3129, -0.4807,  0.1477,  0.1623, -0.7589,  0.2420, -0.1196,\n",
       "          0.2329, -0.0635,  0.3093, -0.1157,  0.0907,  0.2149,  0.1216, -0.1242,\n",
       "         -0.5222,  0.0860, -0.0279,  0.2214, -0.3305,  0.1967, -0.1007, -0.0972,\n",
       "         -0.5381, -0.1814,  0.2152,  0.1256, -0.0513,  0.3066,  0.2815, -0.2962,\n",
       "          0.0243,  0.1938,  0.2016, -0.0797,  0.0613, -0.0596,  0.0259,  0.0251,\n",
       "          0.1348,  0.0294,  0.2524, -0.0850,  0.1291,  0.4252, -0.2034,  0.1876,\n",
       "          0.4001, -0.0131, -0.3261,  0.0826, -0.3660,  0.1048,  0.1448,  0.1681,\n",
       "          0.2545, -0.0747,  0.0079, -0.1651,  0.1351, -0.2896, -0.0269,  0.1322,\n",
       "         -0.1772, -0.5123, -0.1496, -0.0466, -0.0386,  0.3497,  0.0188, -0.3253,\n",
       "         -0.0391, -1.2786,  0.0740,  0.0891, -0.2276, -0.0467, -0.1803,  0.0478,\n",
       "          0.2119, -0.0972, -0.1642,  0.2027, -0.0637, -0.1057, -0.1908, -0.2675,\n",
       "         -0.1163, -0.0290,  0.0557,  0.1442, -0.0584,  0.0298, -0.1370, -0.1201,\n",
       "         -0.2363, -0.1605,  0.0192,  0.4683,  0.1541,  0.0260, -0.0393, -0.1492,\n",
       "          0.4535, -0.0200,  0.1929, -0.0364,  0.0504,  0.0698,  0.1726,  0.4713,\n",
       "          0.6057,  0.1718,  0.2963,  0.0232, -0.2016,  0.1112,  0.2606,  0.0437,\n",
       "         -0.9784, -0.3100,  0.0634, -0.0358, -0.0250, -0.0813,  0.6175, -0.2207,\n",
       "          0.0574, -0.2226,  0.0988, -0.1953,  0.0108,  0.1136, -0.1160,  0.1952,\n",
       "         -0.1843, -0.0632,  0.0122,  0.2135, -0.2401, -0.3710, -0.3364, -0.0619,\n",
       "         -0.1215,  0.0510,  0.0487,  0.4251,  0.2553,  0.1644,  0.1198, -0.0474,\n",
       "         -0.0582,  0.3405,  0.3196, -0.0263, -0.1665, -0.0537,  0.1735, -0.0886,\n",
       "         -0.1973,  0.0745,  0.4098, -0.3109, -0.0009, -0.0796, -0.1276, -0.1258,\n",
       "          0.0338, -0.1011,  0.0112,  0.2663,  0.2301,  0.0090, -0.0605, -0.2365,\n",
       "          0.0516,  0.0405, -0.0370,  0.2068, -0.4674,  0.0803, -0.0561,  0.1587,\n",
       "          0.0847,  0.0475,  0.0668,  0.3873,  0.3093, -0.0634,  0.1449,  0.2112,\n",
       "         -0.1326, -0.0101,  0.3461,  0.0400, -0.0168,  0.4297, -0.2545,  0.2750,\n",
       "         -0.2493,  0.0034, -0.1674, -0.2307,  0.1581, -0.1692, -0.0416,  0.0043,\n",
       "         -0.0416,  0.0298, -0.2387,  0.3678, -0.7438,  0.1156,  0.2408, -0.0807,\n",
       "          0.0889, -0.1401, -0.1289, -0.0487,  0.2169, -0.0058,  0.0875,  0.2197,\n",
       "          0.1515,  0.0597,  0.0462, -0.2927,  0.1321, -0.0453, -0.0832, -0.1075,\n",
       "         -0.2094, -0.0474, -0.1285,  0.7615,  0.0485,  0.0286, -0.1282, -0.3553,\n",
       "          0.2318,  0.2795,  0.3848,  0.3784, -0.2518, -0.0821, -0.0535, -0.0030,\n",
       "         -0.5946,  0.3637, -0.0414, -0.0777,  0.2882, -0.0743, -0.0724, -0.2235,\n",
       "         -0.2132, -0.2781,  0.3916, -0.2656, -4.2541,  0.1567,  0.2184, -0.4873,\n",
       "          0.1030, -0.4616,  0.0063,  0.1265, -0.3889, -0.3063,  0.0463,  0.2015,\n",
       "          0.4455,  0.1656, -0.1785, -0.4261, -0.0602, -0.0415, -0.3543,  0.0028,\n",
       "         -0.2353,  0.3913, -0.0498, -0.1107, -0.1242,  0.1050, -0.1619, -0.2764,\n",
       "          0.1824, -0.1841,  0.0350,  0.0653, -0.1349,  0.0856, -0.1784, -0.1152,\n",
       "          0.2499,  0.6566, -0.0420, -0.1472, -0.2994,  0.0359, -0.2383,  0.0198,\n",
       "         -0.0311,  0.3847, -0.3558,  0.4033,  0.0335, -0.2199, -0.3049, -0.0101,\n",
       "         -0.1339, -0.2800, -0.2444,  0.0799, -0.1330,  0.0513, -0.0114, -0.1042,\n",
       "         -0.1316, -0.2055,  0.0897, -0.1108, -0.2442,  0.0247, -0.2090, -0.3152,\n",
       "          0.1969,  0.0632,  0.0541,  0.1482, -0.1949, -0.5687,  0.0101,  0.1551,\n",
       "          0.1361, -0.2917, -0.0794,  0.1423,  0.0378, -0.0450, -0.1242, -0.2501,\n",
       "         -0.2608, -0.0592,  0.2605, -0.2089, -0.3387,  0.1785,  0.0615,  0.0023,\n",
       "          0.0794,  0.1054,  0.0112, -0.3128, -0.0880, -0.1021, -0.0753, -0.0778,\n",
       "          0.2165, -0.0744, -0.3362, -0.0402,  0.0147,  0.1568,  0.1102, -0.2201,\n",
       "          0.0723,  0.2183, -0.1286, -0.2888, -0.0093, -0.1806, -0.0234, -0.1780,\n",
       "          0.0356,  0.2141, -0.2642, -0.0201,  0.0160, -0.0638,  0.0971,  0.1629,\n",
       "         -0.2051, -0.2504,  0.2816,  0.3017,  0.2034, -0.0126,  0.0962, -0.1208,\n",
       "          0.0494,  0.1517,  0.1902, -0.2971,  0.0988,  0.0968, -0.4395,  0.3888,\n",
       "          0.1556, -0.1216, -0.0657, -0.0493, -0.3143,  0.0760,  0.3864, -0.2035,\n",
       "          0.0871,  0.0891, -0.1411,  0.0119,  0.1142,  0.7041, -0.2790,  0.0990,\n",
       "         -0.2746, -0.2605, -0.4223, -0.1614, -0.0289,  0.0934, -0.2063,  0.0166,\n",
       "          0.0289,  0.1173, -0.1090, -0.3411, -0.1902, -0.0397, -0.0828, -0.0317,\n",
       "          0.0966, -0.4370,  0.0589, -0.2265,  0.0014, -0.2970,  0.1833,  0.0704,\n",
       "         -0.4273, -0.0919,  0.0136, -0.2161,  0.5114, -0.1818, -0.3348, -0.0579,\n",
       "         -0.3463,  0.1262, -0.0638, -0.1252,  0.0867,  0.1610, -0.0060, -0.3139,\n",
       "         -0.1392, -0.2955,  0.0102, -0.1444, -0.1659,  0.0763, -0.3324, -0.0518,\n",
       "         -0.1034,  0.1737, -0.2910, -0.3686,  0.2021, -0.5404,  0.1462, -0.0622,\n",
       "         -0.2842,  0.1338, -0.1008, -0.2522, -0.2275,  0.1189, -0.0442, -0.3320,\n",
       "         -0.0786, -0.1486, -0.2415,  0.0428,  0.1544, -0.3172, -0.1984,  0.1964,\n",
       "         -0.1268,  0.2419,  0.1483,  0.1789,  0.3478, -0.8088, -0.1050,  0.1834,\n",
       "         -0.0341, -0.8509, -0.1914, -0.0308,  0.1473, -0.2762,  0.1083, -0.1264,\n",
       "         -0.2086,  0.2645,  0.0803,  0.1678, -0.0450,  0.0550,  0.0543,  0.0313,\n",
       "         -0.2376, -0.1263,  0.0982,  0.1023,  0.1866,  0.1343,  0.0128, -0.0526,\n",
       "          0.0889, -0.0546,  0.2850, -0.2453,  0.2342, -0.0621,  0.1056,  0.1175,\n",
       "          0.3685,  0.1921, -0.3287,  0.0356,  0.0483,  0.0109,  0.2611,  0.1003,\n",
       "          0.1365, -0.5382,  0.0416,  0.2971,  0.0641, -0.2795, -0.1054,  0.1422,\n",
       "         -0.2158, -0.1010, -0.1003, -0.2898,  0.1196, -0.0281,  0.0711,  0.0671,\n",
       "         -0.1137, -0.1420, -0.1493,  0.5752, -0.2904,  0.4055, -0.2448, -0.4520,\n",
       "          0.1553, -0.1132, -0.0521,  0.0535, -0.1243,  0.2031, -0.1065, -0.0972,\n",
       "         -0.3705,  0.0842, -0.2222, -0.0034, -0.1520,  0.2201, -0.1333,  0.1178,\n",
       "         -0.3498, -0.1117, -0.2608,  0.1252,  0.0758,  0.0528,  0.1319,  0.3134,\n",
       "         -0.2338, -0.1033,  0.1958, -0.0049, -0.2643,  0.0815,  0.2632,  0.0063,\n",
       "          0.2106, -0.1913, -0.0994, -0.1559, -0.0535,  0.0033, -0.2603,  0.0519,\n",
       "         -0.0167,  0.0347, -0.1375,  0.1594, -0.2416,  0.4378, -0.3730, -0.2346,\n",
       "          0.0956,  0.0340,  0.0028, -0.0737, -0.2050,  0.1576,  0.0910, -0.0474,\n",
       "          0.1108,  0.3264, -0.0456, -0.0065, -0.1391, -0.1065, -0.2553,  0.2440,\n",
       "         -0.4553, -0.1948, -0.4954, -0.0643,  0.1065, -0.3065, -0.0817,  0.1577,\n",
       "          0.0222, -0.0608, -0.1258,  0.2730,  0.2988, -0.2006,  0.0065, -0.2963,\n",
       "         -0.2438, -0.0939,  0.2282,  0.1659,  0.0730,  0.1748, -0.1380,  0.3419,\n",
       "         -0.1891, -0.1120, -0.0984,  0.3151,  0.3901,  0.1276,  0.0306,  0.1218,\n",
       "         -0.2710,  0.2797,  0.4077,  0.1317,  0.0406,  0.1817,  0.1540,  0.1499,\n",
       "         -0.3806, -0.1993, -0.2256, -0.1492, -0.2423, -0.0945,  0.0174,  0.0769,\n",
       "         -0.0290, -0.0938,  0.1194,  0.4389, -0.0683, -0.0172, -0.1483, -0.3244,\n",
       "          0.1696,  0.0693, -0.1210,  0.0603, -0.0983,  0.7325,  0.2068, -0.0754,\n",
       "          0.0799,  0.2747, -0.2630,  0.1169,  0.1378, -0.0042, -0.0335, -0.1340,\n",
       "          0.0758,  0.1367, -0.1915, -0.5645,  0.3899, -0.1079, -0.2202, -0.3620,\n",
       "         -0.0531,  0.1042, -0.0072, -0.0573, -0.3517,  0.2880, -0.0816,  0.1554,\n",
       "          0.1646,  0.0098,  0.0921, -0.1294,  0.0227, -0.0416,  0.1283,  0.0298]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.2497, -0.2608,  0.4228,  0.1481,  0.2560, -0.2400,  0.0675,  0.6774,\n",
       "         -0.2617,  0.2635, -0.0048, -0.1584, -0.6036,  0.4951, -0.0106,  0.8101,\n",
       "          0.1081,  0.1142, -0.2973,  0.6830,  0.5105, -0.0659,  0.1132,  0.3114,\n",
       "          0.4098, -0.1282,  0.0599, -0.1679, -0.4782,  0.0365,  0.5590,  0.0988,\n",
       "         -0.2772, -0.2289,  0.4038,  0.2477, -0.3453,  0.0540,  0.3237,  0.0352,\n",
       "         -0.4339, -0.7116, -0.4096,  0.5067,  0.3141, -0.3890,  0.4667,  0.1607,\n",
       "         -0.1111,  0.0586, -0.2740,  0.2439, -0.2019, -0.6187,  0.4109,  0.4841,\n",
       "         -0.3722, -0.8074, -0.1909, -0.5252,  0.0999, -0.2436,  0.5261, -0.3647,\n",
       "          0.3348, -0.0462, -0.2086,  0.4089, -0.8180, -0.3386, -0.3974, -0.2547,\n",
       "         -0.1708,  0.0749, -0.0274, -0.2568, -0.3363,  0.4073, -0.1258, -0.0276,\n",
       "          0.1476,  0.6075, -0.0536,  0.6096,  0.0131,  0.2635,  0.2509,  0.2679,\n",
       "          0.0215,  0.6494,  0.1804,  0.0337,  0.6152, -0.1040, -0.0813,  0.0083,\n",
       "          0.2014, -0.1073, -0.3727,  0.3170,  0.0673, -0.3518,  0.6969,  0.1610,\n",
       "         -0.2057,  0.2155,  0.4462,  0.0596,  0.0286,  0.2415,  0.1353, -0.1035,\n",
       "         -0.2302, -0.3714, -0.0940,  0.2706,  0.0181,  0.3357, -0.0474, -0.2365,\n",
       "          0.3497,  0.1504, -0.2526,  1.1524, -0.0030,  0.3950,  0.2293,  0.3027,\n",
       "         -0.3882, -0.3247,  0.1529,  0.3058, -0.0881, -0.4008, -0.3385,  0.1278,\n",
       "         -0.0159, -0.1236,  0.4079,  0.1528,  0.1786, -0.5345,  0.2087,  0.1869,\n",
       "          0.4161, -0.0783, -0.1245,  0.4198, -0.0839, -0.0551,  0.1524, -0.1264,\n",
       "          0.1569, -0.3476, -0.0329, -0.1264,  0.0655,  0.0713,  0.1901,  0.4425,\n",
       "          0.4321, -0.2911, -0.1297,  0.4322, -0.1490,  0.1276, -0.0185,  0.5289,\n",
       "         -0.1624,  0.1720, -0.2904,  0.1712,  0.8487, -0.4382,  0.3129,  0.0923,\n",
       "          0.0677, -0.1549, -0.1484,  0.6778, -0.8198,  0.2198,  0.1670,  0.3170,\n",
       "         -0.0275, -0.2707,  0.2945,  0.2513, -0.0634, -0.5883,  0.0381, -0.4597,\n",
       "         -0.0791,  0.1700,  0.0416,  0.2257,  0.0625,  0.3307, -0.0951,  0.1350,\n",
       "         -0.0246, -0.0300,  0.0477,  0.0934, -0.4772,  0.2101,  0.1865, -0.3580,\n",
       "         -0.4311, -0.1140, -0.3683,  0.1470,  0.0159,  0.0151,  0.0099, -0.1213,\n",
       "         -0.3474, -0.2183, -0.3057,  0.4462,  0.2258, -0.0412, -0.4363,  0.0381,\n",
       "         -0.1398,  0.8763, -0.2063, -0.3199, -0.4955,  0.0892,  0.1043, -0.1967,\n",
       "          0.7494,  0.1001,  0.1234,  0.5043, -0.0202, -0.2765,  0.3326, -0.0531,\n",
       "         -0.0733,  0.5307,  0.5833, -0.1712, -0.3258,  0.2665, -0.0953,  0.2409,\n",
       "          0.0534, -0.1419, -0.4033, -0.2076, -0.1023, -0.5228,  0.1331, -0.2066,\n",
       "          0.2400,  0.1196, -0.1210, -0.0089,  0.1301,  0.3085,  0.2719, -0.0179,\n",
       "          0.0178, -0.6525, -0.3605,  0.4546,  0.2665,  0.2206,  0.2942, -0.2859,\n",
       "         -0.0576,  0.4691, -0.1927, -0.4943, -0.0351,  0.3151, -0.3213, -0.2151,\n",
       "         -0.1681,  0.2879, -0.2502,  0.0878, -0.3997,  0.2936,  0.2091, -0.0585,\n",
       "         -0.2580, -0.0263, -0.0340,  0.1501, -0.0961, -0.1883,  0.0323,  0.4008,\n",
       "          0.0187,  0.3952,  0.2475, -0.2826,  0.0553,  0.1087,  0.0156, -0.0259,\n",
       "         -0.1306, -0.0028,  0.3988, -0.8792, -3.4857,  0.2564, -0.0210,  0.3468,\n",
       "          0.0085, -0.2445, -0.2185, -0.3671, -0.6558, -0.0903, -0.0771, -0.1843,\n",
       "          0.4539, -0.1214, -0.1933, -0.0730,  0.1234, -0.6052, -0.1087,  0.5659,\n",
       "         -0.2042, -0.2760, -0.0451,  0.1489,  0.7972,  0.4670,  0.0835, -0.2122,\n",
       "          0.0801, -0.3799, -0.0476, -0.0537, -0.0216, -0.3421, -0.0222,  0.0596,\n",
       "          0.4541, -0.3436, -0.2600, -0.2589, -0.3465, -0.7269, -0.0780, -0.2095,\n",
       "          0.6700, -0.0079, -0.1900, -0.1626, -0.3370, -0.0119,  0.2001, -0.1606,\n",
       "         -0.1383,  0.0570, -0.0553,  0.1021,  0.4593,  0.0705, -0.2308, -0.2369,\n",
       "         -0.0023,  0.1820, -0.1185, -0.1474, -0.5549, -0.5706, -0.3337, -0.4960,\n",
       "          0.2729,  0.0644, -0.0911,  0.4109, -0.2922, -0.5983,  0.0237, -0.5007,\n",
       "         -0.0284, -0.2740, -0.3416, -0.2273, -0.3507, -0.4336,  0.3856, -0.0240,\n",
       "         -0.0984, -0.4633,  0.1160, -0.0783, -0.4009, -0.4210,  0.0736, -0.1784,\n",
       "          0.0113,  0.0136,  0.2532,  0.0457, -0.0265, -0.0233,  0.2352,  0.1911,\n",
       "         -0.1630,  0.1638,  0.5054, -0.3369, -0.1838,  0.1046, -0.1197,  0.3325,\n",
       "         -0.3598,  0.1653,  0.1622, -0.3293,  0.2731, -0.3059, -0.3971, -0.3217,\n",
       "          0.5341,  1.0976,  0.1149,  0.0639, -0.1764,  0.3535, -0.5802,  0.1271,\n",
       "          0.1195,  0.2652, -0.0172,  0.3705,  0.1382, -0.2567, -0.1733, -0.3705,\n",
       "         -0.0366, -0.2588,  0.3341,  0.2582,  0.0298, -0.2438, -0.6192,  0.7113,\n",
       "          0.2871,  0.0448,  0.2059, -0.0319, -0.1078,  0.1442,  0.2317,  0.2488,\n",
       "         -0.2650,  0.1194, -0.6017, -0.1957,  0.1688, -0.2326,  0.3792,  0.3515,\n",
       "          0.5164,  0.0282, -0.2619, -0.8606, -0.1963,  0.0748, -0.1352, -0.1307,\n",
       "         -0.3328,  0.4202, -0.1145, -0.2210, -0.5487,  0.5731, -0.1323, -0.0383,\n",
       "          0.1931, -0.1466, -0.3389, -0.0640, -0.1682, -0.4710, -0.2215,  0.0504,\n",
       "         -0.3221,  0.0508, -0.0080, -0.3590,  0.1868, -0.1046,  0.1163, -0.3090,\n",
       "         -0.2404,  0.0953,  0.2792,  0.2942, -0.1583, -0.0056, -0.1083, -0.2806,\n",
       "         -0.0759, -0.2350,  0.2651,  0.4681,  0.3495,  0.2292, -0.3384,  0.0529,\n",
       "         -0.4642, -0.1696, -0.0608, -0.0925, -0.2418,  0.2214, -0.3010, -0.0102,\n",
       "         -0.6808,  0.0176, -0.0443, -0.3337, -0.4077, -0.0056, -0.4090, -0.1948,\n",
       "         -0.2689, -0.1890,  0.2063,  0.4651, -0.5017, -0.4415, -0.3524,  0.1459,\n",
       "         -0.2664, -0.2221, -0.3002, -0.2771, -0.0375,  0.1698, -0.0876,  0.0612,\n",
       "         -0.0936, -0.4364, -0.3519,  0.0710, -0.0856, -0.0065, -0.2514, -0.2944,\n",
       "          0.0429,  0.0022, -0.2326,  0.3234, -0.1937,  0.5244, -0.1789,  0.5126,\n",
       "         -0.0647, -0.2572,  0.0698, -0.7121, -0.0884, -0.0399, -0.1426, -0.1036,\n",
       "         -0.2383,  0.0410,  0.0253,  0.0705,  0.2859,  0.0078,  0.0013,  0.1029,\n",
       "         -0.1160, -0.2910, -0.4149, -0.1088,  0.1005,  0.0090, -0.1599,  0.0648,\n",
       "          0.0077, -0.5504,  0.5000,  0.0592, -0.3473,  0.0441, -0.0339, -0.1648,\n",
       "          0.0261, -0.4277,  0.5085,  0.4236, -0.1293, -0.5872,  0.3387,  0.3457,\n",
       "         -0.1881,  0.6331,  0.0112,  0.2184,  0.4780,  0.1767, -0.1719, -0.2513,\n",
       "         -0.0640, -0.1645,  0.3554,  0.1210, -0.0426, -0.1821, -0.1367, -0.5354,\n",
       "         -0.0946,  0.2170, -0.0640, -0.4419,  0.6349,  0.1859, -0.4085,  0.4038,\n",
       "         -0.0016, -0.0867,  0.1361,  0.1021,  0.0848,  0.3042,  0.4353,  0.1525,\n",
       "          0.3522,  0.0805, -0.1123,  0.4089,  0.0643,  0.2551, -0.3291,  0.0047,\n",
       "         -0.1220,  0.5649, -0.2227, -0.4560, -0.0710,  0.2845, -0.0522,  0.4471,\n",
       "          0.3031, -0.0771,  0.0772,  0.3243,  0.2557,  0.1858, -0.0075, -0.2435,\n",
       "          0.3854,  0.2334,  0.4112, -0.0911, -0.4754,  0.3597,  0.5518, -0.5224,\n",
       "          0.4215,  0.2495, -0.1175,  0.4706,  0.0038, -0.0505,  0.4019, -0.5938,\n",
       "          0.2258,  0.3508,  0.1397, -0.3859, -0.0779,  0.3108,  0.3451, -0.1604,\n",
       "         -0.4747, -0.2775, -0.0091,  0.1250, -0.2899, -0.4201, -0.3429,  0.0288,\n",
       "         -0.3679, -0.0778,  0.0567, -0.0778, -0.2743,  0.0156, -0.4185, -0.3487,\n",
       "         -0.2747, -0.3140, -0.1951,  0.1277,  0.3939,  0.0321,  0.0239, -0.2912,\n",
       "          0.3507,  0.3780,  0.2365, -0.2882,  0.3098, -0.2293, -0.1346,  0.1070,\n",
       "          0.0350, -0.3994,  0.0229,  0.3324, -0.9545, -0.1458, -0.1635,  0.4507,\n",
       "         -0.3096,  0.4236,  0.1852,  0.3448,  0.3093,  0.0679, -0.4654,  0.0533,\n",
       "         -0.2833, -0.5500, -0.0816,  0.2206, -0.4904,  1.1444,  0.2856,  0.5539,\n",
       "          0.1417, -0.0781, -0.2376,  0.4600,  0.2863, -0.1689, -0.2344,  0.0440,\n",
       "         -0.4242,  0.3116, -0.4276, -0.2753,  0.3500, -0.2513, -0.1737, -0.3454,\n",
       "          0.1671,  0.0110,  0.1982, -0.0707,  0.2662, -0.2151, -0.0747,  0.3797,\n",
       "         -0.0059, -0.4579,  0.1872, -0.9342,  0.4942, -0.2522, -0.3292,  0.0886]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "second_to_last"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'module' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-22-fa09e77282ed>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtorch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlast_four_sum\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: 'module' object is not callable"
     ]
    }
   ],
   "source": [
    "torch(last_four_sum[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3072"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(last_four_cat[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Our final sentence embedding vector of shape: 768\n",
      "Our final sentence embedding vector of shape: 768\n",
      "Our final sentence embedding vector of shape: 768\n",
      "Our final sentence embedding vector of shape: 3072\n",
      "Our final sentence embedding vector of shape: 768\n"
     ]
    }
   ],
   "source": [
    "print (\"Our final sentence embedding vector of shape:\", first[0].shape[0])\n",
    "print (\"Our final sentence embedding vector of shape:\", second_to_last[0].shape[0])\n",
    "print (\"Our final sentence embedding vector of shape:\", last_four_sum[0].shape[0])\n",
    "print (\"Our final sentence embedding vector of shape:\", last_four_cat[0].shape[0])\n",
    "print (\"Our final sentence embedding vector of shape:\", sum_all[0].shape[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 768])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "first.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 768])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "second_to_last.size()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 768])"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "last_four_sum[0].view(1,-1).size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 768])"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "first.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "only one element tensors can be converted to Python scalars",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-42-5714dfc630e9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFloatTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlast_four_sum\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m: only one element tensors can be converted to Python scalars"
     ]
    }
   ],
   "source": [
    "torch.FloatTensor([last_four_sum[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 3])"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r1 = torch.Tensor(2, 3)\n",
    "r1.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0000, 0.0000, 0.0000],\n",
       "        [0.0000, 0.0000, 0.0000]])"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.stack([torch.Tensor(3), torch.Tensor(3)])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
